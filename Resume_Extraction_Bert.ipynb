{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Resume_Extraction_Bert.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"75985ba6cd654a09a7b4781ea173dd1e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_419537d1e11742a880e38170cd7867c8","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_eebda69d0bc6446684981fa8750a7d85","IPY_MODEL_f0f5062cdce84bd2942e8378bfe91ce3"]}},"419537d1e11742a880e38170cd7867c8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"eebda69d0bc6446684981fa8750a7d85":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_cdef0019ae2e4be58dbc032c9941f765","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":570,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":570,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_215930109e9f494ab17c7dca7f00f2f4"}},"f0f5062cdce84bd2942e8378bfe91ce3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_5f9f638a1b2140269e9dc32fffea94b6","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 570/570 [00:00&lt;00:00, 1.03kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_eaf9aa061a8f4b5d8bf301fe508429e5"}},"cdef0019ae2e4be58dbc032c9941f765":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"215930109e9f494ab17c7dca7f00f2f4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5f9f638a1b2140269e9dc32fffea94b6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"eaf9aa061a8f4b5d8bf301fe508429e5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a692f4b74ced4678be6fbdee645db51e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_218732e9537d4171ba5d6472c88293af","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_33eda3a85c2540ec8473c1c6a45ba51d","IPY_MODEL_c07014f7a9d14d78b9f5d758b49b15ff"]}},"218732e9537d4171ba5d6472c88293af":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"33eda3a85c2540ec8473c1c6a45ba51d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_60f0c0fb6f25422d8cc69c9573f8be82","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":440473133,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":440473133,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_35816e6a4dcf47d18b6a753e1c801a68"}},"c07014f7a9d14d78b9f5d758b49b15ff":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_8805643f884d476bb751ed91944e454a","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 440M/440M [00:14&lt;00:00, 30.6MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d426f60479f6420696244a95993b49fb"}},"60f0c0fb6f25422d8cc69c9573f8be82":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"35816e6a4dcf47d18b6a753e1c801a68":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8805643f884d476bb751ed91944e454a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"d426f60479f6420696244a95993b49fb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"8hY3lGS51cy0"},"source":["! pip install pdfminer.six"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EarnKf2qSrN5"},"source":["! pip install  seqeval transformers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w4CGj9iaSr-m"},"source":["import re\n","import json\n","import logging\n","import numpy as np\n","from tqdm import trange\n","import torch\n","from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import confusion_matrix\n","\n","\n","def convert_goldparse(dataturks_JSON_FilePath):\n","    try:\n","        training_data = []\n","        lines = []\n","        with open(dataturks_JSON_FilePath, 'r') as f:\n","            lines = f.readlines()\n","\n","        for line in lines:\n","            data = json.loads(line)\n","            text = data['content'].replace(\"\\n\", \" \")\n","            entities = []\n","            data_annotations = data['annotation']\n","            if data_annotations is not None:\n","                for annotation in data_annotations:\n","                    point = annotation['points'][0]\n","                    labels = annotation['label']\n","                    if not isinstance(labels, list):\n","                        labels = [labels]\n","\n","                    for label in labels:\n","                        point_start = point['start']\n","                        point_end = point['end']\n","                        point_text = point['text']\n","\n","                        lstrip_diff = len(point_text) - \\\n","                            len(point_text.lstrip())\n","                        rstrip_diff = len(point_text) - \\\n","                            len(point_text.rstrip())\n","                        if lstrip_diff != 0:\n","                            point_start = point_start + lstrip_diff\n","                        if rstrip_diff != 0:\n","                            point_end = point_end - rstrip_diff\n","                        entities.append((point_start, point_end + 1, label))\n","            training_data.append((text, {\"entities\": entities}))\n","        return training_data\n","    except Exception as e:\n","        logging.exception(\"Unable to process \" +\n","                          dataturks_JSON_FilePath + \"\\n\" + \"error = \" + str(e))\n","        return None\n","\n","\n","def trim_entity_spans(data: list) -> list:\n","    \"\"\"Removes leading and trailing white spaces from entity spans.\n","\n","    Args:\n","        data (list): The data to be cleaned in spaCy JSON format.\n","\n","    Returns:\n","        list: The cleaned data.\n","    \"\"\"\n","    invalid_span_tokens = re.compile(r'\\s')\n","\n","    cleaned_data = []\n","    for text, annotations in data:\n","        entities = annotations['entities']\n","        valid_entities = []\n","        for start, end, label in entities:\n","            valid_start = start\n","            valid_end = end\n","            while valid_start < len(text) and invalid_span_tokens.match(\n","                    text[valid_start]):\n","                valid_start += 1\n","            while valid_end > 1 and invalid_span_tokens.match(\n","                    text[valid_end - 1]):\n","                valid_end -= 1\n","            valid_entities.append([valid_start, valid_end, label])\n","        cleaned_data.append([text, {'entities': valid_entities}])\n","    return cleaned_data\n","\n","\n","def get_label(offset, labels):\n","    if offset[0] == 0 and offset[1] == 0:\n","        return 'O'\n","    for label in labels:\n","        if offset[1] >= label[0] and offset[0] <= label[1]:\n","            return label[2]\n","    return 'O'\n","\n","\n","tags_vals = [\"UNKNOWN\", \"O\", \"Name\", \"Degree\", \"Skills\", \"College Name\", \"Email Address\",\n","             \"Designation\", \"Companies worked at\", \"Graduation Year\", \"Years of Experience\", \"Location\"]\n","\n","tag2idx = {t: i for i, t in enumerate(tags_vals)}\n","idx2tag = {i: t for i, t in enumerate(tags_vals)}\n","\n","\n","def process_resume(data, tokenizer, tag2idx, max_len, is_test=False):\n","    tok = tokenizer.encode_plus(\n","        data[0], max_length=max_len, pad_to_max_length=True,return_offsets_mapping=True)\n","    curr_sent = {'orig_labels': [], 'labels': []}\n","\n","    padding_length = max_len - len(tok['input_ids'])\n","\n","    if not is_test:\n","        labels = data[1]['entities']\n","        labels.reverse()\n","        for off in tok['offset_mapping']:\n","            label = get_label(off, labels)\n","            curr_sent['orig_labels'].append(label)\n","            curr_sent['labels'].append(tag2idx[label])\n","        curr_sent['labels'] = curr_sent['labels'] + ([0] * padding_length)\n","\n","    curr_sent['input_ids'] = tok['input_ids'] + ([0] * padding_length)\n","    curr_sent['token_type_ids'] = tok['token_type_ids'] + \\\n","        ([0] * padding_length)\n","    curr_sent['attention_mask'] = tok['attention_mask'] + \\\n","        ([0] * padding_length)\n","    return curr_sent\n","\n","\n","class ResumeDataset(Dataset):\n","    def __init__(self, resume, tokenizer, tag2idx, max_len, is_test=False):\n","        self.resume = resume\n","        self.tokenizer = tokenizer\n","        self.is_test = is_test\n","        self.tag2idx = tag2idx\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.resume)\n","\n","    def __getitem__(self, idx):\n","        data = process_resume(\n","            self.resume[idx], self.tokenizer, self.tag2idx, self.max_len, self.is_test)\n","        return {\n","            'input_ids': torch.tensor(data['input_ids'], dtype=torch.long),\n","            'token_type_ids': torch.tensor(data['token_type_ids'], dtype=torch.long),\n","            'attention_mask': torch.tensor(data['attention_mask'], dtype=torch.long),\n","            'labels': torch.tensor(data['labels'], dtype=torch.long),\n","            'orig_label': data['orig_labels']\n","        }\n","\n","\n","def get_hyperparameters(model, ff):\n","\n","    # ff: full_finetuning\n","    if ff:\n","        param_optimizer = list(model.named_parameters())\n","        no_decay = [\"bias\", \"gamma\", \"beta\"]\n","        optimizer_grouped_parameters = [\n","            {\n","                \"params\": [\n","                    p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n","                ],\n","                \"weight_decay_rate\": 0.01,\n","            },\n","            {\n","                \"params\": [\n","                    p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n","                ],\n","                \"weight_decay_rate\": 0.0,\n","            },\n","        ]\n","    else:\n","        param_optimizer = list(model.classifier.named_parameters())\n","        optimizer_grouped_parameters = [\n","            {\"params\": [p for n, p in param_optimizer]}]\n","\n","    return optimizer_grouped_parameters\n","\n","\n","def get_special_tokens(tokenizer, tag2idx):\n","    vocab = tokenizer.get_vocab()\n","    pad_tok = vocab[\"[PAD]\"]\n","    sep_tok = vocab[\"[SEP]\"]\n","    cls_tok = vocab[\"[CLS]\"]\n","    o_lab = tag2idx[\"O\"]\n","\n","    return pad_tok, sep_tok, cls_tok, o_lab\n","\n","\n","def annot_confusion_matrix(valid_tags, pred_tags):\n","    \"\"\"\n","    Create an annotated confusion matrix by adding label\n","    annotations and formatting to sklearn's `confusion_matrix`.\n","    \"\"\"\n","\n","    header = sorted(list(set(valid_tags + pred_tags)))\n","\n","    matrix = confusion_matrix(valid_tags, pred_tags, labels=header)\n","\n","    mat_formatted = [header[i] + \"\\t\\t\\t\" +\n","                     str(row) for i, row in enumerate(matrix)]\n","    content = \"\\t\" + \" \".join(header) + \"\\n\" + \"\\n\".join(mat_formatted)\n","\n","    return content\n","\n","\n","def flat_accuracy(valid_tags, pred_tags):\n","    return (np.array(valid_tags) == np.array(pred_tags)).mean()\n","\n","\n","def train_and_val_model(\n","    model,\n","    tokenizer,\n","    optimizer,\n","    epochs,\n","    idx2tag,\n","    tag2idx,\n","    max_grad_norm,\n","    device,\n","    train_dataloader,\n","    valid_dataloader\n","):\n","\n","    pad_tok, sep_tok, cls_tok, o_lab = get_special_tokens(tokenizer, tag2idx)\n","\n","    epoch = 0\n","    for _ in trange(epochs, desc=\"Epoch\"):\n","        epoch += 1\n","\n","        # Training loop\n","        print(\"Starting training loop.\")\n","        model.train()\n","        tr_loss, tr_accuracy = 0, 0\n","        nb_tr_examples, nb_tr_steps = 0, 0\n","        tr_preds, tr_labels = [], []\n","\n","        for step, batch in enumerate(train_dataloader):\n","            # Add batch to gpu\n","\n","            # batch = tuple(t.to(device) for t in batch)\n","            b_input_ids, b_input_mask, b_labels = batch['input_ids'], batch['attention_mask'], batch['labels']\n","            b_input_ids, b_input_mask, b_labels = b_input_ids.to(\n","                device), b_input_mask.to(device), b_labels.to(device)\n","\n","            # Forward pass\n","            outputs = model(\n","                b_input_ids,\n","                token_type_ids=None,\n","                attention_mask=b_input_mask,\n","                labels=b_labels,\n","            )\n","            loss, tr_logits = outputs[:2]\n","\n","            # Backward pass\n","            loss.backward()\n","\n","            # Compute train loss\n","            tr_loss += loss.item()\n","            nb_tr_examples += b_input_ids.size(0)\n","            nb_tr_steps += 1\n","\n","            # Subset out unwanted predictions on CLS/PAD/SEP tokens\n","            preds_mask = (\n","                (b_input_ids != cls_tok)\n","                & (b_input_ids != pad_tok)\n","                & (b_input_ids != sep_tok)\n","            )\n","\n","            tr_logits = tr_logits.cpu().detach().numpy()\n","            tr_label_ids = torch.masked_select(b_labels, (preds_mask == 1))\n","            preds_mask = preds_mask.cpu().detach().numpy()\n","            tr_batch_preds = np.argmax(tr_logits[preds_mask.squeeze()], axis=1)\n","            tr_batch_labels = tr_label_ids.to(\"cpu\").numpy()\n","            tr_preds.extend(tr_batch_preds)\n","            tr_labels.extend(tr_batch_labels)\n","\n","            # Compute training accuracy\n","            tmp_tr_accuracy = flat_accuracy(tr_batch_labels, tr_batch_preds)\n","            tr_accuracy += tmp_tr_accuracy\n","\n","            # Gradient clipping\n","            torch.nn.utils.clip_grad_norm_(\n","                parameters=model.parameters(), max_norm=max_grad_norm\n","            )\n","\n","            # Update parameters\n","            optimizer.step()\n","            model.zero_grad()\n","\n","        tr_loss = tr_loss / nb_tr_steps\n","        tr_accuracy = tr_accuracy / nb_tr_steps\n","\n","        # Print training loss and accuracy per epoch\n","        print(f\"Train loss: {tr_loss}\")\n","        print(f\"Train accuracy: {tr_accuracy}\")\n","\n","        \"\"\"\n","        Validation loop\n","        \"\"\"\n","        print(\"Starting validation loop.\")\n","\n","        model.eval()\n","        eval_loss, eval_accuracy = 0, 0\n","        nb_eval_steps, nb_eval_examples = 0, 0\n","        predictions, true_labels = [], []\n","\n","        for batch in valid_dataloader:\n","\n","            b_input_ids, b_input_mask, b_labels = batch['input_ids'], batch['attention_mask'], batch['labels']\n","            b_input_ids, b_input_mask, b_labels = b_input_ids.to(\n","                device), b_input_mask.to(device), b_labels.to(device)\n","\n","            with torch.no_grad():\n","                outputs = model(\n","                    b_input_ids,\n","                    token_type_ids=None,\n","                    attention_mask=b_input_mask,\n","                    labels=b_labels,\n","                )\n","                tmp_eval_loss, logits = outputs[:2]\n","\n","            # Subset out unwanted predictions on CLS/PAD/SEP tokens\n","            preds_mask = (\n","                (b_input_ids != cls_tok)\n","                & (b_input_ids != pad_tok)\n","                & (b_input_ids != sep_tok)\n","            )\n","\n","            logits = logits.cpu().detach().numpy()\n","            label_ids = torch.masked_select(b_labels, (preds_mask == 1))\n","            preds_mask = preds_mask.cpu().detach().numpy()\n","            val_batch_preds = np.argmax(logits[preds_mask.squeeze()], axis=1)\n","            val_batch_labels = label_ids.to(\"cpu\").numpy()\n","            predictions.extend(val_batch_preds)\n","            true_labels.extend(val_batch_labels)\n","\n","            tmp_eval_accuracy = flat_accuracy(\n","                val_batch_labels, val_batch_preds)\n","\n","            eval_loss += tmp_eval_loss.mean().item()\n","            eval_accuracy += tmp_eval_accuracy\n","\n","            nb_eval_examples += b_input_ids.size(0)\n","            nb_eval_steps += 1\n","\n","        # Evaluate loss, acc, conf. matrix, and class. report on devset\n","        \n","        pred_tags = [idx2tag[i] for i in predictions]\n","        valid_tags = [idx2tag[i] for i in true_labels]\n","        cl_report = classification_report(valid_tags, pred_tags)\n","        conf_mat = annot_confusion_matrix(valid_tags, pred_tags)\n","        eval_loss = eval_loss / nb_eval_steps\n","        eval_accuracy = eval_accuracy / nb_eval_steps\n","\n","        # Report metrics\n","        print(f\"Validation loss: {eval_loss}\")\n","        print(f\"Validation Accuracy: {eval_accuracy}\")\n","        print(f\"Classification Report:\\n {cl_report}\")\n","        print(f\"Confusion Matrix:\\n {conf_mat}\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jd50QpdYSuFU","executionInfo":{"status":"ok","timestamp":1622613688079,"user_tz":-360,"elapsed":21712,"user":{"displayName":"Md. Sultan Al Rayhan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjhCtlWLVOxTdLze6d85_KK0bT1U2mf6hT5zI3S=s64","userId":"10259408721627297682"}},"outputId":"41c750ec-ddb1-4caf-a767-e1836e8e02e1"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1bKbqhGwSwE8"},"source":["import torch\n","import numpy as np\n","from pdfminer.high_level import extract_text\n","\n","\n","\n","def preprocess_data(data):\n","    text = extract_text(data)\n","    text = text.replace(\"\\n\", \" \")\n","    text = text.replace(\"\\f\", \" \")\n","    return text\n","\n","\n","def tokenize_resume(text, tokenizer, max_len):\n","    tok = tokenizer.encode_plus(\n","        text, max_length=max_len, pad_to_max_length=True, return_offsets_mapping=True)\n","\n","    curr_sent = dict()\n","\n","    padding_length = max_len - len(tok['input_ids'])\n","\n","    curr_sent['input_ids'] = tok['input_ids'] + ([0] * padding_length)\n","    curr_sent['token_type_ids'] = tok['token_type_ids'] + \\\n","        ([0] * padding_length)\n","    curr_sent['attention_mask'] = tok['attention_mask'] + \\\n","        ([0] * padding_length)\n","\n","    final_data = {\n","        'input_ids': torch.tensor(curr_sent['input_ids'], dtype=torch.long),\n","        'token_type_ids': torch.tensor(curr_sent['token_type_ids'], dtype=torch.long),\n","        'attention_mask': torch.tensor(curr_sent['attention_mask'], dtype=torch.long),\n","        'offset_mapping': tok['offset_mapping']\n","    }\n","\n","    return final_data\n","\n","\n","tags_vals = [\"UNKNOWN\", \"O\", \"Name\", \"Degree\", \"Skills\", \"College Name\", \"Email Address\",\n","             \"Designation\", \"Companies worked at\", \"Graduation Year\", \"Years of Experience\", \"Location\"]\n","idx2tag = {i: t for i, t in enumerate(tags_vals)}\n","resticted_lables = [\"UNKNOWN\", \"O\",\"Degree\", \"Skills\", \"College Name\", \"Email Address\",\n","             \"Designation\", \"Companies worked at\", \"Graduation Year\", \"Years of Experience\", \"Location\"]\n","\n","\n","def predict(model, tokenizer, idx2tag, device, test_resume, max_len):\n","    model.eval()\n","    data = tokenize_resume(test_resume, tokenizer, max_len)\n","    input_ids, input_mask = data['input_ids'].unsqueeze(\n","        0), data['attention_mask'].unsqueeze(0)\n","    labels = torch.tensor([1] * input_ids.size(0),\n","                          dtype=torch.long).unsqueeze(0)\n","\n","    input_ids = input_ids.to(device)\n","    input_mask = input_mask.to(device)\n","    labels = labels.to(device)\n","\n","    with torch.no_grad():\n","        outputs = model(\n","            input_ids,\n","            token_type_ids=None,\n","            attention_mask=input_mask,\n","            labels=labels,\n","        )\n","        tmp_eval_loss, logits = outputs[:2]\n","\n","    logits = logits.cpu().detach().numpy()\n","    label_ids = np.argmax(logits, axis=2)\n","\n","    entities = []\n","    for label_id, offset in zip(label_ids[0], data['offset_mapping']):\n","        curr_id = idx2tag[label_id]\n","        curr_start = offset[0]\n","        curr_end = offset[1]\n","        if curr_id not in resticted_lables:\n","            if len(entities) > 0 and entities[-1]['entity'] == curr_id and curr_start - entities[-1]['end'] in [0, 1]:\n","                entities[-1]['end'] = curr_end\n","            else:\n","                entities.append(\n","                    {'entity': curr_id, 'start': curr_start, 'end': curr_end})\n","    for ent in entities:\n","        ent['text'] = test_resume[ent['start']:ent['end']]\n","    return entities\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JPd2xec0VOw9"},"source":["import argparse\n","import numpy as np\n","import torch\n","from transformers import BertForTokenClassification, BertTokenizerFast\n","from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n","from torch.optim import Adam"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"81jU0iZBS7lk","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["75985ba6cd654a09a7b4781ea173dd1e","419537d1e11742a880e38170cd7867c8","eebda69d0bc6446684981fa8750a7d85","f0f5062cdce84bd2942e8378bfe91ce3","cdef0019ae2e4be58dbc032c9941f765","215930109e9f494ab17c7dca7f00f2f4","5f9f638a1b2140269e9dc32fffea94b6","eaf9aa061a8f4b5d8bf301fe508429e5","a692f4b74ced4678be6fbdee645db51e","218732e9537d4171ba5d6472c88293af","33eda3a85c2540ec8473c1c6a45ba51d","c07014f7a9d14d78b9f5d758b49b15ff","60f0c0fb6f25422d8cc69c9573f8be82","35816e6a4dcf47d18b6a753e1c801a68","8805643f884d476bb751ed91944e454a","d426f60479f6420696244a95993b49fb"]},"executionInfo":{"status":"ok","timestamp":1622614061350,"user_tz":-360,"elapsed":292502,"user":{"displayName":"Md. Sultan Al Rayhan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjhCtlWLVOxTdLze6d85_KK0bT1U2mf6hT5zI3S=s64","userId":"10259408721627297682"}},"outputId":"648319cb-7cc7-47d7-fe2b-1c05f333e2e4"},"source":["# parser = argparse.ArgumentParser(description='Train Bert-NER')\n","# parser.add_argument('-e', type=int, default=5, help='number of epochs')\n","# parser.add_argument('-o', type=str, default='.',\n","#                     help='output path to save model state')\n","\n","# args = parser.parse_args().__dict__\n","\n","# output_path = args['o']\n","\n","MAX_LEN = 512\n","EPOCHS = 20\n","MAX_GRAD_NORM = 1.0\n","MODEL_NAME = 'bert-base-uncased'\n","TOKENIZER = BertTokenizerFast('/content/drive/MyDrive/Resume Data/Copy of vocab.txt', lowercase=True)\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","data = trim_entity_spans(convert_goldparse('/content/drive/MyDrive/Resume Data/Copy of Resumes.json'))\n","\n","total = len(data)\n","train_data, val_data = data[:180], data[180:]\n","\n","train_d = ResumeDataset(train_data, TOKENIZER, tag2idx, MAX_LEN)\n","val_d = ResumeDataset(val_data, TOKENIZER, tag2idx, MAX_LEN)\n","\n","train_sampler = RandomSampler(train_d)\n","train_dl = DataLoader(train_d, sampler=train_sampler, batch_size=8)\n","\n","val_dl = DataLoader(val_d, batch_size=8)\n","\n","model = BertForTokenClassification.from_pretrained(\n","    MODEL_NAME, num_labels=len(tag2idx))\n","model.to(DEVICE)\n","optimizer_grouped_parameters = get_hyperparameters(model, True)\n","optimizer = Adam(optimizer_grouped_parameters, lr=3e-5)\n","\n","train_and_val_model(\n","    model,\n","    TOKENIZER,\n","    optimizer,\n","    EPOCHS,\n","    idx2tag,\n","    tag2idx,\n","    MAX_GRAD_NORM,\n","    DEVICE,\n","    train_dl,\n","    val_dl\n",")\n","\n","torch.save(\n","    {\n","        \"model_state_dict\": model.state_dict()\n","    },\n","    f'/content/drive/MyDrive/Resume Data/model-state.bin',\n",")"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"75985ba6cd654a09a7b4781ea173dd1e","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=570.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a692f4b74ced4678be6fbdee645db51e","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n","- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/20 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Starting training loop.\n","Train loss: 0.9055620457815088\n","Train accuracy: 0.7834522757238771\n","Starting validation loop.\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Epoch:   5%|▌         | 1/20 [00:13<04:14, 13.39s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation loss: 0.6617066979408264\n","Validation Accuracy: 0.8316991799301041\n","Classification Report:\n","                      precision    recall  f1-score   support\n","\n","       College Name       0.00      0.00      0.00       208\n","Companies worked at       0.00      0.00      0.00       236\n","             Degree       0.00      0.00      0.00       152\n","        Designation       0.00      0.00      0.00       323\n","      Email Address       0.67      0.64      0.66      1142\n","    Graduation Year       0.00      0.00      0.00        20\n","           Location       0.21      0.03      0.06       175\n","               Name       1.00      0.38      0.56       190\n","                  O       0.84      0.98      0.91     12880\n","             Skills       0.82      0.14      0.24       902\n","Years of Experience       0.00      0.00      0.00        15\n","\n","           accuracy                           0.83     16243\n","          macro avg       0.32      0.20      0.22     16243\n","       weighted avg       0.78      0.83      0.78     16243\n","\n","Confusion Matrix:\n"," \tCollege Name Companies worked at Degree Designation Email Address Graduation Year Location Name O Skills Years of Experience\n","College Name\t\t\t[  0   0   0   0   3   0   0   0 202   3   0]\n","Companies worked at\t\t\t[  0   0   0   0  20   0   0   0 216   0   0]\n","Degree\t\t\t[  0   0   0   0   1   0   0   0 151   0   0]\n","Designation\t\t\t[  0   0   0   0  12   0   0   0 304   7   0]\n","Email Address\t\t\t[  0   0   0   0 729   0   0   0 413   0   0]\n","Graduation Year\t\t\t[ 0  0  0  0  0  0  0  0 20  0  0]\n","Location\t\t\t[  0   0   0   0  11   0   6   0 158   0   0]\n","Name\t\t\t[ 0  0  0  0 32  0 22 73 62  1  0]\n","O\t\t\t[    0     0     0     0   272     0     0     0 12592    16     0]\n","Skills\t\t\t[  0   0   0   0   1   0   0   0 777 124   0]\n","Years of Experience\t\t\t[ 0  0  0  0  1  0  0  0 14  0  0]\n","Starting training loop.\n","Train loss: 0.47958671139634174\n","Train accuracy: 0.8630883814643305\n","Starting validation loop.\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  10%|█         | 2/20 [00:26<03:59, 13.28s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation loss: 0.442998480796814\n","Validation Accuracy: 0.8445956919619755\n","Classification Report:\n","                      precision    recall  f1-score   support\n","\n","       College Name       0.39      0.35      0.37       208\n","Companies worked at       0.68      0.12      0.21       245\n","             Degree       1.00      0.18      0.30       152\n","        Designation       0.98      0.14      0.24       314\n","      Email Address       0.85      0.75      0.79      1142\n","    Graduation Year       0.00      0.00      0.00        20\n","           Location       0.85      0.27      0.41       175\n","               Name       0.74      0.95      0.83       190\n","                  O       0.89      0.93      0.91     12880\n","             Skills       0.42      0.60      0.49       902\n","Years of Experience       0.00      0.00      0.00        15\n","\n","           accuracy                           0.84     16243\n","          macro avg       0.62      0.39      0.41     16243\n","       weighted avg       0.85      0.84      0.83     16243\n","\n","Confusion Matrix:\n"," \tCollege Name Companies worked at Degree Designation Email Address Graduation Year Location Name O Skills Years of Experience\n","College Name\t\t\t[ 72   0   0   0   0   0   0   3 132   1   0]\n","Companies worked at\t\t\t[  2  30   0   1   1   0   1  12 198   0   0]\n","Degree\t\t\t[65  1 27  0  0  0  0  0 51  8  0]\n","Designation\t\t\t[  4   8   0  43   0   0   0  28 231   0   0]\n","Email Address\t\t\t[  0   0   0   0 852   0   0   0 290   0   0]\n","Graduation Year\t\t\t[ 0  0  0  0  0  0  0  0 20  0  0]\n","Location\t\t\t[  0   0   0   0   0   0  47  13 115   0   0]\n","Name\t\t\t[  0   0   0   0   0   0   1 181   8   0   0]\n","O\t\t\t[   42     5     0     0   148     0     6     9 11925   745     0]\n","Skills\t\t\t[  0   0   0   0   0   0   0   0 364 538   0]\n","Years of Experience\t\t\t[ 0  0  0  0  1  0  0  0 14  0  0]\n","Starting training loop.\n","Train loss: 0.3228487553803817\n","Train accuracy: 0.9031599602573166\n","Starting validation loop.\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  15%|█▌        | 3/20 [00:39<03:42, 13.09s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation loss: 0.4626018822193146\n","Validation Accuracy: 0.829826163000137\n","Classification Report:\n","                      precision    recall  f1-score   support\n","\n","       College Name       0.48      0.89      0.63       208\n","Companies worked at       0.50      0.47      0.48       236\n","             Degree       0.80      0.57      0.66       152\n","        Designation       0.81      0.51      0.62       323\n","      Email Address       0.88      0.72      0.79      1142\n","    Graduation Year       0.00      0.00      0.00        20\n","           Location       0.72      0.59      0.65       175\n","               Name       0.87      0.98      0.92       190\n","                  O       0.93      0.86      0.89     12880\n","             Skills       0.34      0.78      0.47       902\n","Years of Experience       0.00      0.00      0.00        15\n","\n","           accuracy                           0.83     16243\n","          macro avg       0.57      0.58      0.56     16243\n","       weighted avg       0.87      0.83      0.84     16243\n","\n","Confusion Matrix:\n"," \tCollege Name Companies worked at Degree Designation Email Address Graduation Year Location Name O Skills Years of Experience\n","College Name\t\t\t[186   0   1   0   0   0   0   1  20   0   0]\n","Companies worked at\t\t\t[ 18 111   0   5   0   0   2   7  88   5   0]\n","Degree\t\t\t[50  0 86  0  0  0  0  0 16  0  0]\n","Designation\t\t\t[  7  41   1 164   0   0   0   4 105   1   0]\n","Email Address\t\t\t[  0   0   0   0 824   0   0   0 318   0   0]\n","Graduation Year\t\t\t[ 1  0  0  0  0  0  0  0 19  0  0]\n","Location\t\t\t[  2   0   0   0   0   0 103   9  61   0   0]\n","Name\t\t\t[  0   1   0   1   0   0   0 187   1   0   0]\n","O\t\t\t[  122    70    14    33   117     0    39     8 11096  1381     0]\n","Skills\t\t\t[  0   0   5   0   0   0   0   0 192 705   0]\n","Years of Experience\t\t\t[ 0  0  0  0  0  0  0  0 15  0  0]\n","Starting training loop.\n","Train loss: 0.24073617160320282\n","Train accuracy: 0.923650799373653\n","Starting validation loop.\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  20%|██        | 4/20 [00:51<03:27, 12.96s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation loss: 0.3637231796979904\n","Validation Accuracy: 0.876437739489627\n","Classification Report:\n","                      precision    recall  f1-score   support\n","\n","       College Name       0.54      0.87      0.67       208\n","Companies worked at       0.50      0.45      0.47       245\n","             Degree       0.91      0.61      0.73       152\n","        Designation       0.86      0.46      0.60       314\n","      Email Address       0.87      0.74      0.80      1142\n","    Graduation Year       0.00      0.00      0.00        20\n","           Location       0.66      0.75      0.70       175\n","               Name       0.94      0.98      0.96       190\n","                  O       0.93      0.93      0.93     12880\n","             Skills       0.52      0.68      0.59       902\n","Years of Experience       0.00      0.00      0.00        15\n","\n","           accuracy                           0.88     16243\n","          macro avg       0.61      0.59      0.59     16243\n","       weighted avg       0.88      0.88      0.88     16243\n","\n","Confusion Matrix:\n"," \tCollege Name Companies worked at Degree Designation Email Address Graduation Year Location Name O Skills Years of Experience\n","College Name\t\t\t[180   0   0   0   0   0   1   0  27   0   0]\n","Companies worked at\t\t\t[  6 111   0   5   0   0   2   4 117   0   0]\n","Degree\t\t\t[37  0 93  0  0  0  0  0 22  0  0]\n","Designation\t\t\t[  0  43   0 146   0   0   0   5 120   0   0]\n","Email Address\t\t\t[  0   0   0   0 847   0   0   0 295   0   0]\n","Graduation Year\t\t\t[ 1  0  0  0  0  0  0  0 19  0  0]\n","Location\t\t\t[  0   0   0   0   0   0 131   2  42   0   0]\n","Name\t\t\t[  0   0   0   0   0   0   1 186   3   0   0]\n","O\t\t\t[  108    69     7    18   127     0    63     1 11928   559     0]\n","Skills\t\t\t[  0   0   2   0   0   0   0   0 289 611   0]\n","Years of Experience\t\t\t[ 0  0  0  0  0  0  0  0 15  0  0]\n","Starting training loop.\n","Train loss: 0.19112160575130713\n","Train accuracy: 0.9395801688928678\n","Starting validation loop.\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  25%|██▌       | 5/20 [01:04<03:13, 12.90s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation loss: 0.3854489535093307\n","Validation Accuracy: 0.862582882265199\n","Classification Report:\n","                      precision    recall  f1-score   support\n","\n","       College Name       0.56      0.87      0.68       208\n","Companies worked at       0.43      0.69      0.53       236\n","             Degree       0.82      0.81      0.81       152\n","        Designation       0.73      0.65      0.68       323\n","      Email Address       0.80      0.79      0.80      1142\n","    Graduation Year       0.00      0.00      0.00        20\n","           Location       0.62      0.78      0.69       175\n","               Name       0.94      0.98      0.96       190\n","                  O       0.95      0.88      0.91     12880\n","             Skills       0.47      0.79      0.59       902\n","Years of Experience       0.00      0.00      0.00        15\n","\n","           accuracy                           0.86     16243\n","          macro avg       0.57      0.66      0.61     16243\n","       weighted avg       0.89      0.86      0.87     16243\n","\n","Confusion Matrix:\n"," \tCollege Name Companies worked at Degree Designation Email Address Graduation Year Location Name O Skills Years of Experience\n","College Name\t\t\t[181   3   7   0   0   0   2   0  15   0   0]\n","Companies worked at\t\t\t[  6 163   0   7   0   0   2   4  54   0   0]\n","Degree\t\t\t[ 20   0 123   0   0   0   0   0   9   0   0]\n","Designation\t\t\t[  0  52   0 209   0   0   0   1  61   0   0]\n","Email Address\t\t\t[  0   0   0   0 904   0   0   0 238   0   0]\n","Graduation Year\t\t\t[ 0  0  0  0  0  0  0  0 20  0  0]\n","Location\t\t\t[  1   0   0   0   0   0 136   0  38   0   0]\n","Name\t\t\t[  0   1   0   1   0   0   0 187   1   0   0]\n","O\t\t\t[  113   160    20    69   225     0    79     6 11392   816     0]\n","Skills\t\t\t[  0   1   0   0   0   0   0   0 188 713   0]\n","Years of Experience\t\t\t[ 0  0  0  2  0  0  0  0 13  0  0]\n","Starting training loop.\n","Train loss: 0.1610758116711741\n","Train accuracy: 0.9487657999194741\n","Starting validation loop.\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  30%|███       | 6/20 [01:17<03:00, 12.91s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation loss: 0.5148054778575897\n","Validation Accuracy: 0.8341089205539742\n","Classification Report:\n","                      precision    recall  f1-score   support\n","\n","       College Name       0.55      0.87      0.67       208\n","Companies worked at       0.50      0.60      0.55       245\n","             Degree       0.81      0.83      0.82       152\n","        Designation       0.76      0.65      0.70       314\n","      Email Address       0.83      0.76      0.79      1142\n","    Graduation Year       0.00      0.00      0.00        20\n","           Location       0.66      0.78      0.71       175\n","               Name       0.96      0.98      0.97       190\n","                  O       0.94      0.85      0.89     12880\n","             Skills       0.35      0.84      0.50       902\n","Years of Experience       0.00      0.00      0.00        15\n","\n","           accuracy                           0.83     16243\n","          macro avg       0.58      0.65      0.60     16243\n","       weighted avg       0.88      0.83      0.85     16243\n","\n","Confusion Matrix:\n"," \tCollege Name Companies worked at Degree Designation Email Address Graduation Year Location Name O Skills Years of Experience\n","College Name\t\t\t[180   0   6   0   0   0   0   0  22   0   0]\n","Companies worked at\t\t\t[  6 147   0   7   0   0   1   3  77   4   0]\n","Degree\t\t\t[ 14   0 126   0   0   0   0   0  12   0   0]\n","Designation\t\t\t[  0  31   4 204   0   0   0   2  69   4   0]\n","Email Address\t\t\t[  0   0   0   0 865   0   0   0 277   0   0]\n","Graduation Year\t\t\t[ 1  0  0  0  0  0  0  0 19  0  0]\n","Location\t\t\t[  8   0   0   0   0   0 136   0  31   0   0]\n","Name\t\t\t[  0   0   0   0   0   0   0 186   4   0   0]\n","O\t\t\t[  117   116    18    58   183     0    70     3 10935  1380     0]\n","Skills\t\t\t[  0   0   1   0   0   0   0   0 139 762   0]\n","Years of Experience\t\t\t[ 0  0  1  0  0  0  0  0 14  0  0]\n","Starting training loop.\n","Train loss: 0.1277233070653418\n","Train accuracy: 0.9595898757410061\n","Starting validation loop.\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  35%|███▌      | 7/20 [01:30<02:48, 12.98s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation loss: 0.3851779639720917\n","Validation Accuracy: 0.8920879224708111\n","Classification Report:\n","                      precision    recall  f1-score   support\n","\n","       College Name       0.67      0.79      0.72       208\n","Companies worked at       0.51      0.50      0.50       236\n","             Degree       0.88      0.86      0.87       152\n","        Designation       0.81      0.60      0.69       323\n","      Email Address       0.84      0.77      0.80      1142\n","    Graduation Year       1.00      0.05      0.10        20\n","           Location       0.73      0.71      0.72       175\n","               Name       0.97      0.98      0.98       190\n","                  O       0.93      0.94      0.93     12880\n","             Skills       0.60      0.66      0.63       902\n","Years of Experience       0.00      0.00      0.00        15\n","\n","           accuracy                           0.89     16243\n","          macro avg       0.72      0.62      0.63     16243\n","       weighted avg       0.89      0.89      0.89     16243\n","\n","Confusion Matrix:\n"," \tCollege Name Companies worked at Degree Designation Email Address Graduation Year Location Name O Skills Years of Experience\n","College Name\t\t\t[164   0   9   0   0   0   0   0  35   0   0]\n","Companies worked at\t\t\t[  3 117   0   4   0   0   2   3 107   0   0]\n","Degree\t\t\t[  7   0 131   0   0   0   0   0  14   0   0]\n","Designation\t\t\t[  0  40   0 195   3   0   0   2  83   0   0]\n","Email Address\t\t\t[  0   0   0   0 875   0   0   0 267   0   0]\n","Graduation Year\t\t\t[ 0  0  0  0  0  1  0  0 19  0  0]\n","Location\t\t\t[  1   0   0   0   0   0 124   0  50   0   0]\n","Name\t\t\t[  0   0   0   0   0   0   0 187   3   0   0]\n","O\t\t\t[   70    72     9    43   161     0    45     0 12085   395     0]\n","Skills\t\t\t[  0   0   0   0   0   0   0   0 309 593   0]\n","Years of Experience\t\t\t[ 0  0  0  0  1  0  0  0 14  0  0]\n","Starting training loop.\n","Train loss: 0.10370375475157863\n","Train accuracy: 0.9681373349239321\n","Starting validation loop.\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  40%|████      | 8/20 [01:43<02:34, 12.91s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation loss: 0.46301079988479615\n","Validation Accuracy: 0.878938669140506\n","Classification Report:\n","                      precision    recall  f1-score   support\n","\n","       College Name       0.64      0.79      0.71       208\n","Companies worked at       0.54      0.47      0.50       245\n","             Degree       0.98      0.79      0.87       152\n","        Designation       0.89      0.52      0.65       314\n","      Email Address       0.87      0.70      0.77      1142\n","    Graduation Year       0.75      0.15      0.25        20\n","           Location       0.73      0.74      0.73       175\n","               Name       0.97      0.98      0.98       190\n","                  O       0.92      0.93      0.93     12880\n","             Skills       0.51      0.72      0.60       902\n","Years of Experience       0.00      0.00      0.00        15\n","\n","           accuracy                           0.88     16243\n","          macro avg       0.71      0.62      0.64     16243\n","       weighted avg       0.89      0.88      0.88     16243\n","\n","Confusion Matrix:\n"," \tCollege Name Companies worked at Degree Designation Email Address Graduation Year Location Name O Skills Years of Experience\n","College Name\t\t\t[165   0   3   0   0   0   0   0  40   0   0]\n","Companies worked at\t\t\t[  5 114   0   3   0   0   1   3 119   0   0]\n","Degree\t\t\t[ 11   0 120   0   0   0   0   0  21   0   0]\n","Designation\t\t\t[  0  37   0 163   0   0   0   2 112   0   0]\n","Email Address\t\t\t[  0   0   0   0 797   0   0   0 345   0   0]\n","Graduation Year\t\t\t[ 0  0  0  0  0  3  0  0 17  0  0]\n","Location\t\t\t[  0   0   0   0   0   0 129   0  46   0   0]\n","Name\t\t\t[  0   0   0   0   0   0   0 186   4   0   0]\n","O\t\t\t[   75    61     0    18   124     1    47     0 11938   616     0]\n","Skills\t\t\t[  0   1   0   0   0   0   0   0 253 648   0]\n","Years of Experience\t\t\t[ 0  0  0  0  0  0  0  0 15  0  0]\n","Starting training loop.\n","Train loss: 0.0920901230495909\n","Train accuracy: 0.9711245396274637\n","Starting validation loop.\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  45%|████▌     | 9/20 [01:55<02:21, 12.83s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation loss: 0.5326636373996735\n","Validation Accuracy: 0.8528441615120587\n","Classification Report:\n","                      precision    recall  f1-score   support\n","\n","       College Name       0.59      0.87      0.70       208\n","Companies worked at       0.42      0.67      0.52       236\n","             Degree       0.90      0.86      0.88       152\n","        Designation       0.74      0.62      0.68       323\n","      Email Address       0.79      0.79      0.79      1142\n","    Graduation Year       0.33      0.15      0.21        20\n","           Location       0.70      0.77      0.73       175\n","               Name       0.96      0.98      0.97       190\n","                  O       0.94      0.87      0.91     12880\n","             Skills       0.41      0.79      0.54       902\n","Years of Experience       0.67      0.27      0.38        15\n","\n","           accuracy                           0.85     16243\n","          macro avg       0.68      0.69      0.66     16243\n","       weighted avg       0.88      0.85      0.86     16243\n","\n","Confusion Matrix:\n"," \tCollege Name Companies worked at Degree Designation Email Address Graduation Year Location Name O Skills Years of Experience\n","College Name\t\t\t[180   0   4   0   0   0   0   0  24   0   0]\n","Companies worked at\t\t\t[  6 159   0   4   0   0   2   3  61   1   0]\n","Degree\t\t\t[ 10   0 130   0   0   0   0   0  12   0   0]\n","Designation\t\t\t[  0  48   0 201   0   0   0   1  69   4   0]\n","Email Address\t\t\t[  0   0   0   0 906   0   0   0 236   0   0]\n","Graduation Year\t\t\t[ 0  0  0  0  0  3  0  0 17  0  0]\n","Location\t\t\t[  4   0   0   0   0   0 135   0  36   0   0]\n","Name\t\t\t[  0   0   0   0   0   0   0 186   4   0   0]\n","O\t\t\t[  103   168    11    65   239     6    57     4 11221  1004     2]\n","Skills\t\t\t[  0   0   0   0   0   0   0   0 193 709   0]\n","Years of Experience\t\t\t[ 0  0  0  0  0  0  0  0 11  0  4]\n","Starting training loop.\n","Train loss: 0.07164377087484235\n","Train accuracy: 0.9792140103461908\n","Starting validation loop.\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  50%|█████     | 10/20 [02:08<02:07, 12.79s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation loss: 0.5577559053897858\n","Validation Accuracy: 0.8580799844603251\n","Classification Report:\n","                      precision    recall  f1-score   support\n","\n","       College Name       0.56      0.89      0.69       208\n","Companies worked at       0.45      0.66      0.54       245\n","             Degree       0.86      0.88      0.87       152\n","        Designation       0.75      0.62      0.68       314\n","      Email Address       0.75      0.84      0.79      1142\n","    Graduation Year       0.26      0.30      0.28        20\n","           Location       0.71      0.79      0.75       175\n","               Name       0.94      0.98      0.96       190\n","                  O       0.95      0.88      0.91     12880\n","             Skills       0.44      0.72      0.54       902\n","Years of Experience       0.53      0.53      0.53        15\n","\n","           accuracy                           0.86     16243\n","          macro avg       0.65      0.74      0.69     16243\n","       weighted avg       0.88      0.86      0.87     16243\n","\n","Confusion Matrix:\n"," \tCollege Name Companies worked at Degree Designation Email Address Graduation Year Location Name O Skills Years of Experience\n","College Name\t\t\t[185   0   4   0   0   0   0   0  19   0   0]\n","Companies worked at\t\t\t[  5 162   0   6   0   0   1   3  68   0   0]\n","Degree\t\t\t[  8   0 134   0   0   0   0   0  10   0   0]\n","Designation\t\t\t[  0  47   0 194   2   0   0   1  66   4   0]\n","Email Address\t\t\t[  0   0   0   0 958   0   0   0 184   0   0]\n","Graduation Year\t\t\t[ 0  0  0  0  0  6  0  0 14  0  0]\n","Location\t\t\t[  3   0   0   0   0   0 139   0  33   0   0]\n","Name\t\t\t[  0   0   0   0   0   0   0 187   3   0   0]\n","O\t\t\t[  129   149    17    58   324    17    55     7 11292   825     7]\n","Skills\t\t\t[  0   0   0   0   0   0   0   0 256 646   0]\n","Years of Experience\t\t\t[0 0 0 2 1 0 0 0 4 0 8]\n","Starting training loop.\n","Train loss: 0.06524016967286235\n","Train accuracy: 0.9814287880201522\n","Starting validation loop.\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  55%|█████▌    | 11/20 [02:21<01:56, 12.91s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation loss: 0.5489378273487091\n","Validation Accuracy: 0.870209489562846\n","Classification Report:\n","                      precision    recall  f1-score   support\n","\n","       College Name       0.56      0.89      0.69       208\n","Companies worked at       0.43      0.66      0.52       236\n","             Degree       0.88      0.89      0.89       152\n","        Designation       0.79      0.61      0.69       323\n","      Email Address       0.80      0.79      0.80      1142\n","    Graduation Year       0.28      0.35      0.31        20\n","           Location       0.70      0.77      0.73       175\n","               Name       0.95      1.00      0.98       190\n","                  O       0.94      0.90      0.92     12880\n","             Skills       0.49      0.72      0.58       902\n","Years of Experience       0.80      0.53      0.64        15\n","\n","           accuracy                           0.87     16243\n","          macro avg       0.69      0.74      0.70     16243\n","       weighted avg       0.89      0.87      0.88     16243\n","\n","Confusion Matrix:\n"," \tCollege Name Companies worked at Degree Designation Email Address Graduation Year Location Name O Skills Years of Experience\n","College Name\t\t\t[186   0   5   0   0   0   0   0  17   0   0]\n","Companies worked at\t\t\t[  3 156   0   2   0   0   2   3  70   0   0]\n","Degree\t\t\t[  9   0 136   0   0   0   0   0   7   0   0]\n","Designation\t\t\t[  0  50   0 198   0   0   0   1  69   5   0]\n","Email Address\t\t\t[  0   0   0   0 907   0   0   0 235   0   0]\n","Graduation Year\t\t\t[ 0  0  0  0  0  7  0  0 13  0  0]\n","Location\t\t\t[  4   0   0   0   0   0 134   1  36   0   0]\n","Name\t\t\t[  0   0   0   0   0   0   0 190   0   0   0]\n","O\t\t\t[  128   158    14    48   231    18    55     4 11553   669     2]\n","Skills\t\t\t[  0   0   0   0   0   0   0   0 256 646   0]\n","Years of Experience\t\t\t[0 0 0 2 0 0 0 0 5 0 8]\n","Starting training loop.\n","Train loss: 0.05778106075266133\n","Train accuracy: 0.983889285896472\n","Starting validation loop.\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  60%|██████    | 12/20 [02:34<01:43, 12.90s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation loss: 0.5769694089889527\n","Validation Accuracy: 0.8615741478486371\n","Classification Report:\n","                      precision    recall  f1-score   support\n","\n","       College Name       0.65      0.84      0.74       208\n","Companies worked at       0.51      0.55      0.53       245\n","             Degree       0.89      0.89      0.89       152\n","        Designation       0.81      0.62      0.70       314\n","      Email Address       0.82      0.77      0.80      1142\n","    Graduation Year       0.22      0.30      0.26        20\n","           Location       0.74      0.80      0.77       175\n","               Name       0.99      0.98      0.98       190\n","                  O       0.94      0.89      0.91     12880\n","             Skills       0.42      0.77      0.55       902\n","Years of Experience       0.83      0.33      0.48        15\n","\n","           accuracy                           0.86     16243\n","          macro avg       0.71      0.70      0.69     16243\n","       weighted avg       0.89      0.86      0.87     16243\n","\n","Confusion Matrix:\n"," \tCollege Name Companies worked at Degree Designation Email Address Graduation Year Location Name O Skills Years of Experience\n","College Name\t\t\t[175   0   4   0   0   1   0   0  28   0   0]\n","Companies worked at\t\t\t[  5 134   0   4   0   0   1   1 100   0   0]\n","Degree\t\t\t[  5   0 136   0   0   0   0   0  11   0   0]\n","Designation\t\t\t[  0  30   0 194   0   0   0   0  86   4   0]\n","Email Address\t\t\t[  0   0   0   0 883   0   0   0 259   0   0]\n","Graduation Year\t\t\t[ 0  0  0  0  0  6  0  0 14  0  0]\n","Location\t\t\t[  0   0   0   0   0   0 140   0  35   0   0]\n","Name\t\t\t[  0   0   0   0   0   0   0 186   4   0   0]\n","O\t\t\t[   83   101    13    42   191    20    49     1 11422   957     1]\n","Skills\t\t\t[  0   0   0   0   0   0   0   0 204 698   0]\n","Years of Experience\t\t\t[ 0  0  0  0  0  0  0  0 10  0  5]\n","Starting training loop.\n","Train loss: 0.04334632355881774\n","Train accuracy: 0.987330209839214\n","Starting validation loop.\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  65%|██████▌   | 13/20 [02:47<01:30, 12.87s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation loss: 0.677402138710022\n","Validation Accuracy: 0.8528887375328008\n","Classification Report:\n","                      precision    recall  f1-score   support\n","\n","       College Name       0.58      0.88      0.70       208\n","Companies worked at       0.51      0.41      0.45       236\n","             Degree       0.88      0.90      0.89       152\n","        Designation       0.83      0.52      0.64       323\n","      Email Address       0.79      0.82      0.81      1142\n","    Graduation Year       0.28      0.40      0.33        20\n","           Location       0.78      0.72      0.75       175\n","               Name       0.97      0.99      0.98       190\n","                  O       0.94      0.87      0.91     12880\n","             Skills       0.39      0.79      0.52       902\n","Years of Experience       1.00      0.40      0.57        15\n","\n","           accuracy                           0.85     16243\n","          macro avg       0.72      0.70      0.69     16243\n","       weighted avg       0.88      0.85      0.86     16243\n","\n","Confusion Matrix:\n"," \tCollege Name Companies worked at Degree Designation Email Address Graduation Year Location Name O Skills Years of Experience\n","College Name\t\t\t[183   0   5   0   0   1   0   0  19   0   0]\n","Companies worked at\t\t\t[  4  96   0   2   0   0   2   3 128   1   0]\n","Degree\t\t\t[  6   0 137   0   0   0   0   0   9   0   0]\n","Designation\t\t\t[  0  35   2 169   0   0   0   1 111   5   0]\n","Email Address\t\t\t[  0   0   0   0 942   0   5   0 195   0   0]\n","Graduation Year\t\t\t[ 0  0  0  0  0  8  0  0 12  0  0]\n","Location\t\t\t[  7   0   0   0   0   0 126   0  42   0   0]\n","Name\t\t\t[  0   0   0   0   0   0   0 189   1   0   0]\n","O\t\t\t[  113    59    12    30   249    20    29     1 11258  1109     0]\n","Skills\t\t\t[  0   0   0   0   0   0   0   0 191 711   0]\n","Years of Experience\t\t\t[0 0 0 2 0 0 0 0 7 0 6]\n","Starting training loop.\n","Train loss: 0.04293130791705588\n","Train accuracy: 0.987007552726142\n","Starting validation loop.\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  70%|███████   | 14/20 [03:00<01:17, 12.84s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation loss: 0.6539891421794891\n","Validation Accuracy: 0.8538651253566474\n","Classification Report:\n","                      precision    recall  f1-score   support\n","\n","       College Name       0.59      0.88      0.71       208\n","Companies worked at       0.43      0.60      0.50       245\n","             Degree       0.88      0.88      0.88       152\n","        Designation       0.80      0.60      0.68       314\n","      Email Address       0.78      0.80      0.79      1142\n","    Graduation Year       0.23      0.35      0.28        20\n","           Location       0.59      0.79      0.68       175\n","               Name       0.94      1.00      0.97       190\n","                  O       0.94      0.88      0.91     12880\n","             Skills       0.42      0.72      0.53       902\n","Years of Experience       0.53      0.53      0.53        15\n","\n","           accuracy                           0.85     16243\n","          macro avg       0.65      0.73      0.68     16243\n","       weighted avg       0.88      0.85      0.86     16243\n","\n","Confusion Matrix:\n"," \tCollege Name Companies worked at Degree Designation Email Address Graduation Year Location Name O Skills Years of Experience\n","College Name\t\t\t[182   0   5   0   0   1   0   0  20   0   0]\n","Companies worked at\t\t\t[  5 147   0   4   0   0   1   3  84   1   0]\n","Degree\t\t\t[ 10   0 133   0   0   0   0   0   9   0   0]\n","Designation\t\t\t[  0  44   0 187   0   0   0   0  79   4   0]\n","Email Address\t\t\t[  0   0   0   0 915   0  24   0 203   0   0]\n","Graduation Year\t\t\t[ 0  0  0  0  0  7  0  0 13  0  0]\n","Location\t\t\t[  3   0   0   0   0   0 138   0  34   0   0]\n","Name\t\t\t[  0   0   0   0   0   0   0 190   0   0   0]\n","O\t\t\t[  107   150    14    41   260    22    70     9 11295   905     7]\n","Skills\t\t\t[  0   0   0   0   0   0   0   0 256 646   0]\n","Years of Experience\t\t\t[0 0 0 2 0 0 0 0 5 0 8]\n","Starting training loop.\n","Train loss: 0.031306177377700806\n","Train accuracy: 0.9908404792660508\n","Starting validation loop.\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  75%|███████▌  | 15/20 [03:13<01:04, 12.85s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation loss: 0.663453608751297\n","Validation Accuracy: 0.8622551423698225\n","Classification Report:\n","                      precision    recall  f1-score   support\n","\n","       College Name       0.58      0.86      0.69       208\n","Companies worked at       0.40      0.57      0.47       236\n","             Degree       0.88      0.89      0.88       152\n","        Designation       0.84      0.52      0.64       323\n","      Email Address       0.75      0.89      0.82      1142\n","    Graduation Year       0.24      0.40      0.30        20\n","           Location       0.69      0.82      0.75       175\n","               Name       0.94      1.00      0.97       190\n","                  O       0.95      0.88      0.91     12880\n","             Skills       0.44      0.74      0.55       902\n","Years of Experience       0.40      0.53      0.46        15\n","\n","           accuracy                           0.86     16243\n","          macro avg       0.65      0.74      0.68     16243\n","       weighted avg       0.89      0.86      0.87     16243\n","\n","Confusion Matrix:\n"," \tCollege Name Companies worked at Degree Designation Email Address Graduation Year Location Name O Skills Years of Experience\n","College Name\t\t\t[178   0   9   0   0   1   0   0  20   0   0]\n","Companies worked at\t\t\t[  4 134   0   2   0   0   2   3  91   0   0]\n","Degree\t\t\t[  6   0 135   0   0   0   0   0  11   0   0]\n","Designation\t\t\t[  0  64   0 168   0   0   0   2  83   6   0]\n","Email Address\t\t\t[   0    0    0    0 1016    0    4    0  122    0    0]\n","Graduation Year\t\t\t[ 0  0  0  0  0  8  0  0 12  0  0]\n","Location\t\t\t[  4   0   0   0   0   0 143   0  28   0   0]\n","Name\t\t\t[  0   0   0   0   0   0   0 190   0   0   0]\n","O\t\t\t[  113   133    10    28   330    24    59     7 11334   830    12]\n","Skills\t\t\t[  0   0   0   0   0   0   0   0 235 667   0]\n","Years of Experience\t\t\t[0 0 0 2 0 0 0 0 5 0 8]\n","Starting training loop.\n","Train loss: 0.03294392829032048\n","Train accuracy: 0.9906509576576299\n","Starting validation loop.\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  80%|████████  | 16/20 [03:25<00:51, 12.77s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation loss: 0.6642807126045227\n","Validation Accuracy: 0.8611886041901139\n","Classification Report:\n","                      precision    recall  f1-score   support\n","\n","       College Name       0.63      0.86      0.73       208\n","Companies worked at       0.44      0.54      0.48       245\n","             Degree       0.90      0.89      0.90       152\n","        Designation       0.82      0.60      0.69       314\n","      Email Address       0.77      0.82      0.79      1142\n","    Graduation Year       0.27      0.30      0.29        20\n","           Location       0.70      0.80      0.75       175\n","               Name       1.00      0.99      0.99       190\n","                  O       0.94      0.88      0.91     12880\n","             Skills       0.43      0.77      0.56       902\n","Years of Experience       0.42      0.53      0.47        15\n","\n","           accuracy                           0.86     16243\n","          macro avg       0.67      0.73      0.69     16243\n","       weighted avg       0.89      0.86      0.87     16243\n","\n","Confusion Matrix:\n"," \tCollege Name Companies worked at Degree Designation Email Address Graduation Year Location Name O Skills Years of Experience\n","College Name\t\t\t[179   0   6   0   0   1   0   0  22   0   0]\n","Companies worked at\t\t\t[  5 132   0   4   0   0   1   0 102   1   0]\n","Degree\t\t\t[  4   0 136   0   0   0   0   0  12   0   0]\n","Designation\t\t\t[  0  41   0 188   0   0   0   0  80   5   0]\n","Email Address\t\t\t[  0   0   0   0 939   0   6   0 197   0   0]\n","Graduation Year\t\t\t[ 0  0  0  0  0  6  0  0 14  0  0]\n","Location\t\t\t[  2   0   0   0   0   0 140   0  33   0   0]\n","Name\t\t\t[  0   0   0   0   0   0   0 188   2   0   0]\n","O\t\t\t[   95   128     9    34   286    15    52     0 11355   895    11]\n","Skills\t\t\t[  0   0   0   0   0   0   0   0 209 693   0]\n","Years of Experience\t\t\t[0 0 0 2 0 0 0 0 5 0 8]\n","Starting training loop.\n","Train loss: 0.031126553881103577\n","Train accuracy: 0.9906656486851\n","Starting validation loop.\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  85%|████████▌ | 17/20 [03:38<00:38, 12.75s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation loss: 0.6542452692985534\n","Validation Accuracy: 0.8650340089828445\n","Classification Report:\n","                      precision    recall  f1-score   support\n","\n","       College Name       0.58      0.88      0.69       208\n","Companies worked at       0.48      0.58      0.53       236\n","             Degree       0.89      0.89      0.89       152\n","        Designation       0.84      0.57      0.68       323\n","      Email Address       0.85      0.78      0.81      1142\n","    Graduation Year       0.30      0.35      0.33        20\n","           Location       0.74      0.77      0.75       175\n","               Name       0.97      0.99      0.98       190\n","                  O       0.94      0.89      0.92     12880\n","             Skills       0.43      0.75      0.54       902\n","Years of Experience       0.86      0.40      0.55        15\n","\n","           accuracy                           0.86     16243\n","          macro avg       0.72      0.71      0.70     16243\n","       weighted avg       0.89      0.86      0.87     16243\n","\n","Confusion Matrix:\n"," \tCollege Name Companies worked at Degree Designation Email Address Graduation Year Location Name O Skills Years of Experience\n","College Name\t\t\t[182   0   7   0   0   0   0   0  19   0   0]\n","Companies worked at\t\t\t[  6 137   0   2   0   0   2   3  81   5   0]\n","Degree\t\t\t[  6   0 135   0   0   0   0   0  11   0   0]\n","Designation\t\t\t[  0  40   0 185   0   0   0   0  90   8   0]\n","Email Address\t\t\t[  0   0   0   0 886   0   0   0 256   0   0]\n","Graduation Year\t\t\t[ 0  0  0  0  0  7  0  0 13  0  0]\n","Location\t\t\t[  4   0   0   0   0   0 135   0  36   0   0]\n","Name\t\t\t[  0   0   0   0   0   0   0 189   1   0   0]\n","O\t\t\t[  118   107    10    30   162    16    46     2 11491   897     1]\n","Skills\t\t\t[  0   0   0   0   0   0   0   0 229 673   0]\n","Years of Experience\t\t\t[0 0 0 2 0 0 0 0 7 0 6]\n","Starting training loop.\n","Train loss: 0.028256290146838062\n","Train accuracy: 0.9920728285633252\n","Starting validation loop.\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  90%|█████████ | 18/20 [03:51<00:25, 12.71s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation loss: 0.6656005442142486\n","Validation Accuracy: 0.8601722273514605\n","Classification Report:\n","                      precision    recall  f1-score   support\n","\n","       College Name       0.60      0.89      0.72       208\n","Companies worked at       0.49      0.60      0.54       245\n","             Degree       0.87      0.86      0.86       152\n","        Designation       0.80      0.55      0.66       314\n","      Email Address       0.83      0.75      0.78      1142\n","    Graduation Year       0.17      0.40      0.24        20\n","           Location       0.67      0.79      0.73       175\n","               Name       0.98      0.99      0.98       190\n","                  O       0.94      0.89      0.91     12880\n","             Skills       0.44      0.79      0.56       902\n","Years of Experience       0.35      0.40      0.38        15\n","\n","           accuracy                           0.86     16243\n","          macro avg       0.65      0.72      0.67     16243\n","       weighted avg       0.89      0.86      0.87     16243\n","\n","Confusion Matrix:\n"," \tCollege Name Companies worked at Degree Designation Email Address Graduation Year Location Name O Skills Years of Experience\n","College Name\t\t\t[186   0   6   0   0   1   0   0  15   0   0]\n","Companies worked at\t\t\t[  5 146   0   4   0   0   3   3  83   1   0]\n","Degree\t\t\t[ 10   0 130   0   0   0   0   0  12   0   0]\n","Designation\t\t\t[  0  49   0 174   0   0   0   0  82   9   0]\n","Email Address\t\t\t[  0   0   0   0 853   0   2   0 287   0   0]\n","Graduation Year\t\t\t[ 0  0  0  0  0  8  0  0 12  0  0]\n","Location\t\t\t[  3   0   0   0   0   0 139   0  33   0   0]\n","Name\t\t\t[  0   0   0   0   0   0   0 188   2   0   0]\n","O\t\t\t[  106   105    12    37   180    37    62     1 11412   917    11]\n","Skills\t\t\t[  0   0   0   0   0   0   0   0 188 714   0]\n","Years of Experience\t\t\t[0 0 1 2 0 0 0 0 6 0 6]\n","Starting training loop.\n","Train loss: 0.02184799293299084\n","Train accuracy: 0.9935641565851713\n","Starting validation loop.\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  95%|█████████▌| 19/20 [04:03<00:12, 12.68s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation loss: 0.6782584965229035\n","Validation Accuracy: 0.8606052299969758\n","Classification Report:\n","                      precision    recall  f1-score   support\n","\n","       College Name       0.59      0.88      0.70       208\n","Companies worked at       0.48      0.54      0.51       236\n","             Degree       0.74      0.89      0.81       152\n","        Designation       0.80      0.61      0.69       323\n","      Email Address       0.79      0.82      0.80      1142\n","    Graduation Year       0.19      0.40      0.25        20\n","           Location       0.72      0.81      0.77       175\n","               Name       0.97      0.98      0.97       190\n","                  O       0.94      0.88      0.91     12880\n","             Skills       0.43      0.77      0.55       902\n","Years of Experience       0.60      0.40      0.48        15\n","\n","           accuracy                           0.86     16243\n","          macro avg       0.66      0.73      0.68     16243\n","       weighted avg       0.89      0.86      0.87     16243\n","\n","Confusion Matrix:\n"," \tCollege Name Companies worked at Degree Designation Email Address Graduation Year Location Name O Skills Years of Experience\n","College Name\t\t\t[182   0  13   0   0   1   0   0  12   0   0]\n","Companies worked at\t\t\t[  7 127   0   2   0   0   2   3  94   1   0]\n","Degree\t\t\t[  5   0 136   0   0   0   0   0  11   0   0]\n","Designation\t\t\t[  0  32   3 198   0   0   0   0  81   9   0]\n","Email Address\t\t\t[  0   0   0   0 933   0   0   0 209   0   0]\n","Graduation Year\t\t\t[ 0  0  0  0  0  8  0  0 12  0  0]\n","Location\t\t\t[  4   0   0   0   0   0 142   0  29   0   0]\n","Name\t\t\t[  0   0   0   0   0   0   0 186   4   0   0]\n","O\t\t\t[  113   103    31    46   245    34    52     3 11347   902     4]\n","Skills\t\t\t[  0   0   0   0   0   0   0   0 208 694   0]\n","Years of Experience\t\t\t[0 0 1 2 0 0 0 0 6 0 6]\n","Starting training loop.\n","Train loss: 0.01993104718301607\n","Train accuracy: 0.9941303833381118\n","Starting validation loop.\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 20/20 [04:16<00:00, 12.83s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation loss: 0.6240813970565796\n","Validation Accuracy: 0.8770425761358054\n","Classification Report:\n","                      precision    recall  f1-score   support\n","\n","       College Name       0.66      0.81      0.73       208\n","Companies worked at       0.49      0.47      0.48       245\n","             Degree       0.87      0.86      0.86       152\n","        Designation       0.84      0.53      0.65       314\n","      Email Address       0.80      0.76      0.78      1142\n","    Graduation Year       0.17      0.35      0.23        20\n","           Location       0.63      0.81      0.71       175\n","               Name       0.96      0.98      0.97       190\n","                  O       0.94      0.91      0.92     12880\n","             Skills       0.52      0.77      0.62       902\n","Years of Experience       0.69      0.60      0.64        15\n","\n","           accuracy                           0.88     16243\n","          macro avg       0.69      0.71      0.69     16243\n","       weighted avg       0.89      0.88      0.88     16243\n","\n","Confusion Matrix:\n"," \tCollege Name Companies worked at Degree Designation Email Address Graduation Year Location Name O Skills Years of Experience\n","College Name\t\t\t[169   0   9   0   0   1   0   0  29   0   0]\n","Companies worked at\t\t\t[  3 115   0   3   0   0   2   3 119   0   0]\n","Degree\t\t\t[  4   0 130   0   0   0   0   0  18   0   0]\n","Designation\t\t\t[  0  46   0 167   0   0   0   1  97   3   0]\n","Email Address\t\t\t[  0   0   0   0 873   0  24   0 245   0   0]\n","Graduation Year\t\t\t[ 0  0  0  0  0  7  0  0 13  0  0]\n","Location\t\t\t[  3   0   0   0   0   0 142   0  30   0   0]\n","Name\t\t\t[  0   0   0   0   0   0   0 187   3   0   0]\n","O\t\t\t[   78    75    10    27   223    32    56     3 11736   636     4]\n","Skills\t\t\t[  0   0   0   0   0   0   0   0 208 694   0]\n","Years of Experience\t\t\t[0 0 0 1 0 0 0 0 5 0 9]\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"4zlN3Co_S_t7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622614073024,"user_tz":-360,"elapsed":5263,"user":{"displayName":"Md. Sultan Al Rayhan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjhCtlWLVOxTdLze6d85_KK0bT1U2mf6hT5zI3S=s64","userId":"10259408721627297682"}},"outputId":"1cbf4e26-51b5-4e79-9c04-20d0afc4e28b"},"source":["import io\n","import argparse\n","import torch\n","from transformers import BertTokenizerFast, BertForTokenClassification\n"," \n","MAX_LEN = 500\n","NUM_LABELS = 12\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","MODEL_PATH = 'bert-base-uncased'\n","STATE_DICT = torch.load(\"/content/drive/MyDrive/Resume Data/model-state.bin\", map_location=DEVICE)\n","TOKENIZER = BertTokenizerFast(\"/content/drive/MyDrive/Resume Data/Copy of vocab.txt\", lowercase=True)\n"," \n","model = BertForTokenClassification.from_pretrained(\n"," 'bert-base-uncased', state_dict=STATE_DICT['model_state_dict'], num_labels=NUM_LABELS)\n","model.to(DEVICE)\n","\n","\n"," \n","data = io.BytesIO(open('/content/drive/MyDrive/Applicants Resume/Android-iOS-Flutter/Shariful Islam.pdf', 'rb').read())\n","resume_text = preprocess_data(data)\n","entities = predict(model, TOKENIZER, idx2tag,\n"," DEVICE, resume_text, MAX_LEN)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2v0jwwIZVfdH","executionInfo":{"status":"ok","timestamp":1622614084986,"user_tz":-360,"elapsed":412,"user":{"displayName":"Md. Sultan Al Rayhan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjhCtlWLVOxTdLze6d85_KK0bT1U2mf6hT5zI3S=s64","userId":"10259408721627297682"}},"outputId":"ee452d10-4672-4e58-ae2c-38989fa1c363"},"source":["entities"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'end': 4, 'entity': 'Name', 'start': 0, 'text': 'Name'},\n"," {'end': 24, 'entity': 'Name', 'start': 6, 'text': 'Md. Shariful Islam'}]"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"a4mdffQNtdnr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622614122905,"user_tz":-360,"elapsed":1418,"user":{"displayName":"Md. Sultan Al Rayhan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjhCtlWLVOxTdLze6d85_KK0bT1U2mf6hT5zI3S=s64","userId":"10259408721627297682"}},"outputId":"e6c6d170-543e-4f6c-a71c-32b3e16dcca8"},"source":["import io\n","data1 = io.BytesIO(open('/content/drive/MyDrive/Applicants Resume/DevOps-IT-QA/Akash kundu.pdf', 'rb').read())\n","resume_text1 = preprocess_data(data1)\n","entities1 = predict(model, TOKENIZER, idx2tag,\n"," DEVICE, resume_text1, MAX_LEN)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"TbXmikIiuYl9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622614131658,"user_tz":-360,"elapsed":425,"user":{"displayName":"Md. Sultan Al Rayhan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjhCtlWLVOxTdLze6d85_KK0bT1U2mf6hT5zI3S=s64","userId":"10259408721627297682"}},"outputId":"995cc61f-4e30-4291-f22e-defaded42d1c"},"source":["entities1"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'end': 5, 'entity': 'Name', 'start': 0, 'text': 'P E R'}]"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p_ljxbfWulOS","executionInfo":{"status":"ok","timestamp":1622614163888,"user_tz":-360,"elapsed":393,"user":{"displayName":"Md. Sultan Al Rayhan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjhCtlWLVOxTdLze6d85_KK0bT1U2mf6hT5zI3S=s64","userId":"10259408721627297682"}},"outputId":"8054de23-4ae3-4663-d3fe-ad030ce59d32"},"source":["data3 = io.BytesIO(open('/content/drive/MyDrive/Applicants Resume/DevOps-IT-QA/Akif Ahmed.pdf', 'rb').read())\n","resume_text3 = preprocess_data(data3)\n","entities3 = predict(model, TOKENIZER, idx2tag,\n"," DEVICE, resume_text3, MAX_LEN)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"WygTul4Tuy0e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622614168914,"user_tz":-360,"elapsed":385,"user":{"displayName":"Md. Sultan Al Rayhan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjhCtlWLVOxTdLze6d85_KK0bT1U2mf6hT5zI3S=s64","userId":"10259408721627297682"}},"outputId":"4ee0bd47-be8c-4975-b3df-4153484b6d07"},"source":["entities3"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'end': 129, 'entity': 'Name', 'start': 108, 'text': 'Syed Akif Ahmed Karim'}]"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ny-jaT6y2RZs","executionInfo":{"status":"ok","timestamp":1621941759228,"user_tz":-360,"elapsed":2350,"user":{"displayName":"Md. Sultan Al Rayhan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjhCtlWLVOxTdLze6d85_KK0bT1U2mf6hT5zI3S=s64","userId":"10259408721627297682"}},"outputId":"a850b4b6-a99b-4aae-8b98-22f6db63da79"},"source":["data3 = io.BytesIO(open('/content/drive/MyDrive/Applicants Resume/DevOps-IT-QA/Ariful Islam.pdf', 'rb').read())\n","resume_text3 = preprocess_data(data3)\n","entities3 = predict(model, TOKENIZER, idx2tag,\n"," DEVICE, resume_text3, MAX_LEN)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"6cj0mUCk3l1b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622614187289,"user_tz":-360,"elapsed":375,"user":{"displayName":"Md. Sultan Al Rayhan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjhCtlWLVOxTdLze6d85_KK0bT1U2mf6hT5zI3S=s64","userId":"10259408721627297682"}},"outputId":"cc684d3c-c8da-4ce3-84c4-413a59b4f8ec"},"source":["entities3"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'end': 129, 'entity': 'Name', 'start': 108, 'text': 'Syed Akif Ahmed Karim'}]"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hBqHR7b8Q0b9","executionInfo":{"status":"ok","timestamp":1622614203358,"user_tz":-360,"elapsed":1032,"user":{"displayName":"Md. Sultan Al Rayhan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjhCtlWLVOxTdLze6d85_KK0bT1U2mf6hT5zI3S=s64","userId":"10259408721627297682"}},"outputId":"8cfe5326-85d3-4d7a-a859-f287845a3ebd"},"source":["data4 = io.BytesIO(open('/content/drive/MyDrive/Applicants Resume/DevOps-IT-QA/Bidhan kumar Bhownick.pdf', 'rb').read())\n","resume_text4 = preprocess_data(data4)\n","entities4 = predict(model, TOKENIZER, idx2tag,\n"," DEVICE, resume_text4, MAX_LEN)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5cAehBcPQ8-d","executionInfo":{"status":"ok","timestamp":1622614207673,"user_tz":-360,"elapsed":374,"user":{"displayName":"Md. Sultan Al Rayhan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjhCtlWLVOxTdLze6d85_KK0bT1U2mf6hT5zI3S=s64","userId":"10259408721627297682"}},"outputId":"0f202d81-480f-4d4b-be8e-7f5e979f490d"},"source":["entities4"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'end': 12, 'entity': 'Name', 'start': 11, 'text': 'B'},\n"," {'end': 44, 'entity': 'Name', 'start': 23, 'text': 'BIDHAN KUMAR BHOWMICK'}]"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9Js7-6GfQ-lq","executionInfo":{"status":"ok","timestamp":1622615163042,"user_tz":-360,"elapsed":1037,"user":{"displayName":"Md. Sultan Al Rayhan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjhCtlWLVOxTdLze6d85_KK0bT1U2mf6hT5zI3S=s64","userId":"10259408721627297682"}},"outputId":"35b2edbf-6617-48c4-8f8d-61de0535e525"},"source":["data5 = io.BytesIO(open('/content/drive/MyDrive/Applicants Resume/DevOps-IT-QA/Diponkor Roy.pdf', 'rb').read())\n","resume_text5 = preprocess_data(data5)\n","entities5 = predict(model, TOKENIZER, idx2tag,\n"," DEVICE, resume_text5, MAX_LEN)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nB_wO-e5RLOt","executionInfo":{"status":"ok","timestamp":1622615165114,"user_tz":-360,"elapsed":391,"user":{"displayName":"Md. Sultan Al Rayhan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjhCtlWLVOxTdLze6d85_KK0bT1U2mf6hT5zI3S=s64","userId":"10259408721627297682"}},"outputId":"ac45f9a3-3d2d-4f31-c1b1-2e29dea9416e"},"source":["entities5"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'end': 12, 'entity': 'Name', 'start': 0, 'text': 'Diponkor Roy'},\n"," {'end': 224, 'entity': 'Name', 'start': 222, 'text': 'on'}]"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cKj8Ymd7RNzM","executionInfo":{"status":"ok","timestamp":1622615188289,"user_tz":-360,"elapsed":1421,"user":{"displayName":"Md. Sultan Al Rayhan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjhCtlWLVOxTdLze6d85_KK0bT1U2mf6hT5zI3S=s64","userId":"10259408721627297682"}},"outputId":"24f65608-41f1-40d2-885b-b4a2cf40534e"},"source":["data6 = io.BytesIO(open('/content/drive/MyDrive/Applicants Resume/DevOps-IT-QA/Jannatul Ferdous Meem.pdf', 'rb').read())\n","resume_text6 = preprocess_data(data6)\n","entities6 = predict(model, TOKENIZER, idx2tag,\n"," DEVICE, resume_text6, MAX_LEN)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qjGNQjKARYQJ","executionInfo":{"status":"ok","timestamp":1622615190308,"user_tz":-360,"elapsed":371,"user":{"displayName":"Md. Sultan Al Rayhan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjhCtlWLVOxTdLze6d85_KK0bT1U2mf6hT5zI3S=s64","userId":"10259408721627297682"}},"outputId":"08995356-22fb-42fd-fd50-f54468f91d94"},"source":["entities6"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'end': 21, 'entity': 'Name', 'start': 0, 'text': 'Jannatul Ferdous[Meem'}]"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"onESXrUgRajE","executionInfo":{"status":"ok","timestamp":1622615204687,"user_tz":-360,"elapsed":1028,"user":{"displayName":"Md. Sultan Al Rayhan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjhCtlWLVOxTdLze6d85_KK0bT1U2mf6hT5zI3S=s64","userId":"10259408721627297682"}},"outputId":"433bdfc1-bb44-4e72-f619-917c545186ff"},"source":["data7 = io.BytesIO(open('/content/drive/MyDrive/Applicants Resume/DevOps-IT-QA/Kamrul Islam.pdf', 'rb').read())\n","resume_text7 = preprocess_data(data7)\n","entities7 = predict(model, TOKENIZER, idx2tag,\n"," DEVICE, resume_text7, MAX_LEN)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IrMWkdEHRl9T","executionInfo":{"status":"ok","timestamp":1622615206062,"user_tz":-360,"elapsed":7,"user":{"displayName":"Md. Sultan Al Rayhan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjhCtlWLVOxTdLze6d85_KK0bT1U2mf6hT5zI3S=s64","userId":"10259408721627297682"}},"outputId":"efaf9a0c-e2c0-45d9-c2e0-74e39287ebaa"},"source":["entities7"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'end': 508, 'entity': 'Name', 'start': 500, 'text': '. KAMRUL'},\n"," {'end': 1678, 'entity': 'Name', 'start': 1676, 'text': 'Ma'},\n"," {'end': 1685, 'entity': 'Name', 'start': 1682, 'text': 'Ash'}]"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SYTqoX-YRnnp","executionInfo":{"status":"ok","timestamp":1622615211440,"user_tz":-360,"elapsed":1014,"user":{"displayName":"Md. Sultan Al Rayhan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjhCtlWLVOxTdLze6d85_KK0bT1U2mf6hT5zI3S=s64","userId":"10259408721627297682"}},"outputId":"ef28cbb8-04c3-4139-be96-bf3425a7ebf5"},"source":["data8 = io.BytesIO(open('/content/drive/MyDrive/Applicants Resume/DevOps-IT-QA/Kibtia Chowdhury.pdf', 'rb').read())\n","resume_text8 = preprocess_data(data8)\n","entities8 = predict(model, TOKENIZER, idx2tag,\n"," DEVICE, resume_text8, MAX_LEN)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2jmQyI01R6Fr","executionInfo":{"status":"ok","timestamp":1622615213442,"user_tz":-360,"elapsed":367,"user":{"displayName":"Md. Sultan Al Rayhan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjhCtlWLVOxTdLze6d85_KK0bT1U2mf6hT5zI3S=s64","userId":"10259408721627297682"}},"outputId":"b9921f4a-0d5a-4101-85fb-2e3b8604faf6"},"source":["entities8"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'end': 43, 'entity': 'Name', 'start': 27, 'text': 'KIBTIA CHOWDHURY'}]"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZFA0mHKMR8Vq","executionInfo":{"status":"ok","timestamp":1622615222547,"user_tz":-360,"elapsed":981,"user":{"displayName":"Md. Sultan Al Rayhan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjhCtlWLVOxTdLze6d85_KK0bT1U2mf6hT5zI3S=s64","userId":"10259408721627297682"}},"outputId":"b22d1e8c-0984-419d-f474-3fc279efcbce"},"source":["data9 = io.BytesIO(open('/content/drive/MyDrive/Applicants Resume/DevOps-IT-QA/MARZAHAN_SULTANA_CV_SQAT.pdf', 'rb').read())\n","resume_text9 = preprocess_data(data9)\n","entities9 = predict(model, TOKENIZER, idx2tag,\n"," DEVICE, resume_text9, MAX_LEN)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e8KASKKrSGbR","executionInfo":{"status":"ok","timestamp":1622615227956,"user_tz":-360,"elapsed":375,"user":{"displayName":"Md. Sultan Al Rayhan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjhCtlWLVOxTdLze6d85_KK0bT1U2mf6hT5zI3S=s64","userId":"10259408721627297682"}},"outputId":"a9c0aabb-7784-40ca-bc42-767d168cc99e"},"source":["entities9"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'end': 24, 'entity': 'Name', 'start': 3, 'text': 'MARZAHAN SULTANA PILI'}]"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AsGUigCUSIQQ","executionInfo":{"status":"ok","timestamp":1622615236817,"user_tz":-360,"elapsed":2033,"user":{"displayName":"Md. Sultan Al Rayhan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjhCtlWLVOxTdLze6d85_KK0bT1U2mf6hT5zI3S=s64","userId":"10259408721627297682"}},"outputId":"2783763a-bf34-4c43-aae0-6dbc7e0fab88"},"source":["data10 = io.BytesIO(open('/content/drive/MyDrive/Applicants Resume/DevOps-IT-QA/MD. ABU MAS-UD SAYEED.pdf', 'rb').read())\n","resume_text10 = preprocess_data(data10)\n","entities10 = predict(model, TOKENIZER, idx2tag,\n"," DEVICE, resume_text10, MAX_LEN)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"817XD9UfSTyk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622616515294,"user_tz":-360,"elapsed":400,"user":{"displayName":"Md. Sultan Al Rayhan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjhCtlWLVOxTdLze6d85_KK0bT1U2mf6hT5zI3S=s64","userId":"10259408721627297682"}},"outputId":"117d3cf2-6cbd-493b-d928-dbc41f6582e1"},"source":["for i in entities9:\n","  if (len(i[\"text\"])>10):\n","    print(i[\"text\"])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["MARZAHAN SULTANA PILI\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fYAwzhXRSWmC"},"source":[""],"execution_count":null,"outputs":[]}]}